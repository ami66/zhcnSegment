# zhcnSegment
中文分词程序，基于结巴分词

最近要写一个计算两个句子相似度的程序，需要分词，因此上网找找有关Python分词的资料。

看了别人的介绍，发现在python分词可选择中科院的分词和结巴分词，由于中科院分词要调用C++，所以我选择了结巴分词



结巴分词速度还行，但没有停用词表。于是我将结巴分词包装了一下，加入百度停用词列表、哈工大停用词表扩展、四川大学机器智能实验室停用词库与中文停用词
库这四个停用词库。


结巴分词GitHub: https://github.com/fxsjy/jieba

我封装后的程序: https://github.com/WenDesi/zhcnSegment



分装程序Demo中对  “他来到了网易杭研大厦”  分词，分别是去除停用词和保留停用词，其结果如下图所示
